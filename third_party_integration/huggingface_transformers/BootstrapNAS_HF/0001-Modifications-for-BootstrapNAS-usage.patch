From 4112c8fbe73464aff52c1e4e53ae3ccf87e11a30 Mon Sep 17 00:00:00 2001
From: "J. Pablo Munoz" <pablo.munoz@intel.com>
Date: Wed, 30 Nov 2022 17:00:14 -0800
Subject: [PATCH] Enable BootstrapNAS

---
 examples/pytorch/language-modeling/run_clm.py |  87 +++++++++--
 examples/pytorch/question-answering/run_qa.py |  76 ++++++++-
 .../pytorch/question-answering/trainer_qa.py  |   5 +-
 .../pytorch/text-classification/run_glue.py   |  92 +++++++++--
 .../pytorch/text-classification/run_xnli.py   |  78 ++++++++--
 .../pytorch/token-classification/run_ner.py   |  94 +++++++++--
 .../nncf_bootstrapnas_bert_config_conll.json  |  65 ++++++++
 .../nncf_bootstrapnas_bert_config_mrpc.json   |  65 ++++++++
 .../nncf_bootstrapnas_bert_config_squad.json  |  69 +++++++++
 .../nncf_bootstrapnas_bert_config_xnli.json   |  65 ++++++++
 src/transformers/__init__.py                  |   4 +-
 src/transformers/modeling_utils.py            |  27 ++++
 src/transformers/models/bert/modeling_bert.py |   4 +-
 .../models/roberta/modeling_roberta.py        |   4 +-
 src/transformers/optimization.py              |  36 ++++-
 src/transformers/pytorch_utils.py             |   2 +
 src/transformers/trainer.py                   | 146 +++++++++++++++---
 src/transformers/trainer_callback.py          |   6 +
 src/transformers/training_args.py             |  58 ++++++-
 src/transformers/utils/__init__.py            |   2 +
 src/transformers/utils/bnas_utils.py          |  97 ++++++++++++
 21 files changed, 984 insertions(+), 98 deletions(-)
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json
 create mode 100644 src/transformers/utils/bnas_utils.py

diff --git a/examples/pytorch/language-modeling/run_clm.py b/examples/pytorch/language-modeling/run_clm.py
index fe03cde7c..7f694f118 100755
--- a/examples/pytorch/language-modeling/run_clm.py
+++ b/examples/pytorch/language-modeling/run_clm.py
@@ -30,6 +30,8 @@ from itertools import chain
 from typing import Optional
 
 import datasets
+import onnx
+import torch
 from datasets import load_dataset
 
 import evaluate
@@ -52,6 +54,8 @@ from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
 
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.23.0")
@@ -373,22 +377,6 @@ def main():
             "You can do it from another script, save it, and load it from here, using --tokenizer_name."
         )
 
-    if model_args.model_name_or_path:
-        model = AutoModelForCausalLM.from_pretrained(
-            model_args.model_name_or_path,
-            from_tf=bool(".ckpt" in model_args.model_name_or_path),
-            config=config,
-            cache_dir=model_args.cache_dir,
-            revision=model_args.model_revision,
-            use_auth_token=True if model_args.use_auth_token else None,
-        )
-    else:
-        model = AutoModelForCausalLM.from_config(config)
-        n_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
-        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")
-
-    model.resize_token_embeddings(len(tokenizer))
-
     # Preprocessing the datasets.
     # First we tokenize all the texts.
     if training_args.do_train:
@@ -503,6 +491,47 @@ def main():
             preds = preds[:, :-1].reshape(-1)
             return metric.compute(predictions=preds, references=labels)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    if model_args.model_name_or_path:
+        retval = AutoModelForCausalLM.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+            nncf_config=nncf_config,
+            nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+        )
+    else:
+        retval = AutoModelForCausalLM.from_config(config)
+        n_params = sum(dict((p.data_ptr(), p.numel()) for p in retval.parameters()).values())
+        logger.info(f"Training new model from scratch - Total size={n_params / 2 ** 20:.2f}M params")
+
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    model.resize_token_embeddings(len(tokenizer))
+
+    if training_args.to_onnx:
+        if nncf_config is not None:
+           compression_ctrl.export_model(training_args.to_onnx)
+        else:
+           model.to('cpu')
+           dummy_tensor = torch.ones([1, config.n_positions], dtype=torch.long)
+           onnx.export(model, dummy_tensor, training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -516,6 +545,7 @@ def main():
         preprocess_logits_for_metrics=preprocess_logits_for_metrics
         if training_args.do_eval and not is_torch_tpu_available()
         else None,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -526,6 +556,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
         metrics = train_result.metrics
@@ -539,6 +571,29 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                try:
+                    perplexity = math.exp(metrics["eval_loss"])
+                except OverflowError:
+                    perplexity = float("inf")
+                return perplexity
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            # Best found subnet
+            elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(best_config)
+            elasticity_ctrl.export_model(osp.join(config.log_dir, "best_subnet.onnx"))
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index 1240623b5..09270dcb2 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -25,6 +25,7 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import torch
 from datasets import load_dataset
 
 import evaluate
@@ -39,6 +40,8 @@ from transformers import (
     HfArgumentParser,
     PreTrainedTokenizerFast,
     TrainingArguments,
+    BNASLRSchedulerArgs,
+    update_bnas_lr_scheduler_kwargs,
     default_data_collator,
     set_seed,
 )
@@ -47,6 +50,10 @@ from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
 from utils_qa import postprocess_qa_predictions
 
+from torch import onnx
+
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.23.0")
@@ -327,14 +334,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -599,6 +598,43 @@ def main():
     def compute_metrics(p: EvalPrediction):
         return metric.compute(predictions=p.predictions, references=p.label_ids)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        nncf_config.register_extra_structs([BNASLRSchedulerArgs(nncf_config)])
+        update_bnas_lr_scheduler_kwargs(nncf_config, training_args)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 384], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = QuestionAnsweringTrainer(
         model=model,
@@ -610,6 +646,7 @@ def main():
         data_collator=data_collator,
         post_process_function=post_processing_function,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -620,6 +657,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
         metrics = train_result.metrics
@@ -632,6 +671,27 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_f1']
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            if training_args.local_rank in [-1, 0]:
+                search_algo.visualize_search_progression()
+                search_algo.search_progression_to_csv()
+                # Best found subnet
+                elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(best_config)
+                elasticity_ctrl.export_model(os.path.join(training_args.output_dir, "best_subnet.onnx"))
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/question-answering/trainer_qa.py b/examples/pytorch/question-answering/trainer_qa.py
index 59d7a084c..c2b161b4f 100644
--- a/examples/pytorch/question-answering/trainer_qa.py
+++ b/examples/pytorch/question-answering/trainer_qa.py
@@ -31,7 +31,7 @@ class QuestionAnsweringTrainer(Trainer):
         self.eval_examples = eval_examples
         self.post_process_function = post_process_function
 
-    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
+    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval", active_subnet=None):
         eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
         eval_dataloader = self.get_eval_dataloader(eval_dataset)
         eval_examples = self.eval_examples if eval_examples is None else eval_examples
@@ -61,6 +61,9 @@ class QuestionAnsweringTrainer(Trainer):
                 if not key.startswith(f"{metric_key_prefix}_"):
                     metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
 
+            if active_subnet is not None:
+                metrics.update(active_subnet)
+
             self.log(metrics)
         else:
             metrics = {}
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index 3eb423f08..4244f5013 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -29,6 +29,8 @@ from datasets import load_dataset
 
 import evaluate
 import transformers
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -39,6 +41,8 @@ from transformers import (
     PretrainedConfig,
     Trainer,
     TrainingArguments,
+    BNASLRSchedulerArgs,
+    update_bnas_lr_scheduler_kwargs,
     default_data_collator,
     set_seed,
 )
@@ -366,15 +370,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
 
     # Preprocessing the raw_datasets
     if data_args.task_name is not None:
@@ -400,12 +395,12 @@ def main():
     # Some models have set the order of the labels to use, so let's make sure we do use it.
     label_to_id = None
     if (
-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
+        config.label2id != PretrainedConfig(num_labels=num_labels).label2id
         and data_args.task_name is not None
         and not is_regression
     ):
         # Some have all caps in their config, some don't.
-        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
+        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}
         if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
             label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
         else:
@@ -418,11 +413,11 @@ def main():
         label_to_id = {v: i for i, v in enumerate(label_list)}
 
     if label_to_id is not None:
-        model.config.label2id = label_to_id
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = label_to_id
+        config.id2label = {id: label for label, id in config.label2id.items()}
     elif data_args.task_name is not None and not is_regression:
-        model.config.label2id = {l: i for i, l in enumerate(label_list)}
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = {l: i for i, l in enumerate(label_list)}
+        config.id2label = {id: label for label, id in config.label2id.items()}
 
     if data_args.max_seq_length > tokenizer.model_max_length:
         logger.warning(
@@ -458,6 +453,48 @@ def main():
             max_train_samples = min(len(train_dataset), data_args.max_train_samples)
             train_dataset = train_dataset.select(range(max_train_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        nncf_config.register_extra_structs([BNASLRSchedulerArgs(nncf_config)])
+        update_bnas_lr_scheduler_kwargs(nncf_config, training_args)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            import torch
+            from torch import onnx
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor),
+                        training_args.to_onnx, opset_version=10)
+
     if training_args.do_eval:
         if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
             raise ValueError("--do_eval requires a validation dataset")
@@ -518,8 +555,10 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+
     # Training
     if training_args.do_train:
         checkpoint = None
@@ -528,6 +567,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         max_train_samples = (
             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
@@ -540,6 +581,27 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            if training_args.local_rank in [-1, 0]:
+                search_algo.visualize_search_progression()
+                search_algo.search_progression_to_csv()
+                # Best found subnet
+                elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(best_config)
+                elasticity_ctrl.export_model(os.path.join(training_args.output_dir, "best_subnet.onnx"))
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/text-classification/run_xnli.py b/examples/pytorch/text-classification/run_xnli.py
index 55523edfc..d34506948 100755
--- a/examples/pytorch/text-classification/run_xnli.py
+++ b/examples/pytorch/text-classification/run_xnli.py
@@ -26,10 +26,13 @@ from typing import Optional
 
 import datasets
 import numpy as np
+import torch
 from datasets import load_dataset
 
 import evaluate
 import transformers
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -39,6 +42,8 @@ from transformers import (
     HfArgumentParser,
     Trainer,
     TrainingArguments,
+    BNASLRSchedulerArgs,
+    update_bnas_lr_scheduler_kwargs,
     default_data_collator,
     set_seed,
 )
@@ -282,15 +287,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
 
     # Preprocessing the datasets
     # Padding strategy
@@ -367,6 +363,45 @@ def main():
     else:
         data_collator = None
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        nncf_config.register_extra_structs([BNASLRSchedulerArgs(nncf_config)])
+        update_bnas_lr_scheduler_kwargs(nncf_config, training_args)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, training_args.max_seq_length], dtype=torch.long)
+            torch.onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -376,8 +411,10 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+
     # Training
     if training_args.do_train:
         checkpoint = None
@@ -386,6 +423,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         max_train_samples = (
             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
@@ -398,6 +437,27 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            if training_args.local_rank in [-1, 0]:
+                search_algo.visualize_search_progression()
+                search_algo.search_progression_to_csv()
+                # Best found subnet
+                elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(best_config)
+                elasticity_ctrl.export_model(os.path.join(training_args.output_dir, "best_subnet.onnx"))
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/token-classification/run_ner.py b/examples/pytorch/token-classification/run_ner.py
index 52cbbb87b..0dc61952e 100755
--- a/examples/pytorch/token-classification/run_ner.py
+++ b/examples/pytorch/token-classification/run_ner.py
@@ -30,7 +30,11 @@ import numpy as np
 from datasets import ClassLabel, load_dataset
 
 import evaluate
+import torch
 import transformers
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
+from torch import onnx
 from transformers import (
     AutoConfig,
     AutoModelForTokenClassification,
@@ -41,6 +45,8 @@ from transformers import (
     PreTrainedTokenizerFast,
     Trainer,
     TrainingArguments,
+    BNASLRSchedulerArgs,
+    update_bnas_lr_scheduler_kwargs,
     set_seed,
 )
 from transformers.trainer_utils import get_last_checkpoint
@@ -366,15 +372,6 @@ def main():
             use_auth_token=True if model_args.use_auth_token else None,
         )
 
-    model = AutoModelForTokenClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -385,25 +382,25 @@ def main():
         )
 
     # Model has labels -> use them.
-    if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
-        if list(sorted(model.config.label2id.keys())) == list(sorted(label_list)):
+    if config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
+        if list(sorted(config.label2id.keys())) == list(sorted(label_list)):
             # Reorganize `label_list` to match the ordering of the model.
             if labels_are_int:
-                label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}
-                label_list = [model.config.id2label[i] for i in range(num_labels)]
+                label_to_id = {i: int(config.label2id[l]) for i, l in enumerate(label_list)}
+                label_list = [config.id2label[i] for i in range(num_labels)]
             else:
-                label_list = [model.config.id2label[i] for i in range(num_labels)]
+                label_list = [config.id2label[i] for i in range(num_labels)]
                 label_to_id = {l: i for i, l in enumerate(label_list)}
         else:
             logger.warning(
                 "Your model seems to have been trained with labels, but they don't match the dataset: ",
-                f"model labels: {list(sorted(model.config.label2id.keys()))}, dataset labels:"
+                f"model labels: {list(sorted(config.label2id.keys()))}, dataset labels:"
                 f" {list(sorted(label_list))}.\nIgnoring the model labels as a result.",
             )
 
     # Set the correspondences label/ID inside the model config
-    model.config.label2id = {l: i for i, l in enumerate(label_list)}
-    model.config.id2label = {i: l for i, l in enumerate(label_list)}
+    config.label2id = {l: i for i, l in enumerate(label_list)}
+    config.id2label = {i: l for i, l in enumerate(label_list)}
 
     # Map that sends B-Xxx label to its I-Xxx counterpart
     b_to_i_label = []
@@ -504,6 +501,45 @@ def main():
     # Data collator
     data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        nncf_config.register_extra_structs([BNASLRSchedulerArgs(nncf_config)])
+        update_bnas_lr_scheduler_kwargs(nncf_config, training_args)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForTokenClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx,
+                        opset_version=10)
+
     # Metrics
     metric = evaluate.load("seqeval")
 
@@ -549,6 +585,7 @@ def main():
         tokenizer=tokenizer,
         data_collator=data_collator,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -559,6 +596,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
@@ -571,6 +610,27 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            if training_args.local_rank in [-1, 0]:
+                search_algo.visualize_search_progression()
+                search_algo.search_progression_to_csv()
+                # Best found subnet
+                elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(best_config)
+                elasticity_ctrl.export_model(os.path.join(training_args.output_dir, "best_subnet.onnx"))
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
new file mode 100644
index 000000000..69494f76b
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
@@ -0,0 +1,65 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 3, "depth_indicator": 1, "width_indicator": 3, "init_lr": 5e-5, "sample_rate": 1}
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        [
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"
+                        ]
+                    ],
+                    "overwrite_groups_widths": [[3072, 2912, 2760]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ]
+                }
+            }
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 4,
+            "ref_acc": 99.17
+        }
+    }
+}
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
new file mode 100644
index 000000000..e30711cd3
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
@@ -0,0 +1,65 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 5, "depth_indicator": 1,  "width_indicator": 3, "init_lr": 5e-5}
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"
+                        ]
+                    ],
+                    "overwrite_groups_widths": [[3072, 2456, 1992]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ]
+                }
+            }
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 4,
+            "ref_acc": 84.56
+        }
+    }
+}
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
new file mode 100644
index 000000000..b4a602470
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
@@ -0,0 +1,69 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 2, "depth_indicator": 1, "width_indicator": 3, "init_lr": 3e-5}
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"],
+                        ["BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"]
+                    ],
+                    "overwrite_groups_widths": [[4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656],
+                                                [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656], [4096, 3272, 2656]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[12]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[13]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0"
+                        ]
+                    ]
+                }
+            }
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 100,
+            "population": 20,
+            "ref_acc": 93.21
+        }
+    }
+}
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json
new file mode 100644
index 000000000..d8a21d969
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json
@@ -0,0 +1,65 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 2, "depth_indicator": 2, "width_indicator": 3, "init_lr": 5e-5}
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"
+                        ]
+                    ],
+                    "overwrite_groups_widths" : [[3072, 2912, 2760]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ]
+                }
+            }
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 5,
+            "ref_acc": 77.68
+        }
+    }
+}
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index ba24b9886..c1d73c6d4 100755
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -466,7 +466,7 @@ _import_structure = {
         "TrainerState",
     ],
     "trainer_utils": ["EvalPrediction", "IntervalStrategy", "SchedulerType", "enable_full_determinism", "set_seed"],
-    "training_args": ["TrainingArguments"],
+    "training_args": ["TrainingArguments", "BNASLRSchedulerArgs", "update_bnas_lr_scheduler_kwargs"],
     "training_args_seq2seq": ["Seq2SeqTrainingArguments"],
     "training_args_tf": ["TFTrainingArguments"],
     "utils": [
@@ -3447,7 +3447,7 @@ if TYPE_CHECKING:
         TrainerState,
     )
     from .trainer_utils import EvalPrediction, IntervalStrategy, SchedulerType, enable_full_determinism, set_seed
-    from .training_args import TrainingArguments
+    from .training_args import TrainingArguments, BNASLRSchedulerArgs, update_bnas_lr_scheduler_kwargs
     from .training_args_seq2seq import Seq2SeqTrainingArguments
     from .training_args_tf import TFTrainingArguments
 
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 5f4fccd33..3bee3521e 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -27,6 +27,9 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import torch
+from nncf.torch.model_creation import create_nncf_network
+from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import \
+    create_compressed_model_from_algo_names
 from packaging import version
 from torch import Tensor, device, nn
 from torch.nn import CrossEntropyLoss
@@ -59,6 +62,7 @@ from .utils import (
     ContextManagers,
     ModelOutput,
     PushToHubMixin,
+    WeightsFlopsCalculatorForTransformer,
     cached_file,
     copy_func,
     download_url,
@@ -1497,6 +1501,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         push_to_hub: bool = False,
         max_shard_size: Union[int, str] = "10GB",
         safe_serialization: bool = False,
+        nncf_compression_state: Dict = None,
         **kwargs,
     ):
         """
@@ -1901,6 +1906,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         load_in_8bit_skip_modules = kwargs.pop("load_in_8bit_skip_modules", None)
         subfolder = kwargs.pop("subfolder", "")
         commit_hash = kwargs.pop("_commit_hash", None)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         if trust_remote_code is True:
             logger.warning(
@@ -2321,6 +2328,16 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
             if dtype_orig is not None:
                 torch.set_default_dtype(dtype_orig)
 
+            if nncf_config is not None and nncf_eval:
+                nncf_network = create_nncf_network(model, nncf_config)
+                algo_name = nncf_config.get('bootstrapNAS', {}).get('training', {}).get('algorithm', 'progressive_shrinking')
+                compression_ctrl, model = create_compressed_model_from_algo_names(nncf_network, nncf_config,
+                                                                                  algo_names=[algo_name])
+                compression_ctrl.multi_elasticity_handler._weights_calc = WeightsFlopsCalculatorForTransformer(
+                                                                            config.num_attention_heads,
+                                                                          )
+                return compression_ctrl, model
+
             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
                 model,
                 state_dict,
@@ -2344,6 +2361,16 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            nncf_network = create_nncf_network(model, nncf_config)
+            algo_name = nncf_config.get('bootstrapNAS', {}).get('training', {}).get('algorithm', 'progressive_shrinking')
+            compression_ctrl, model = create_compressed_model_from_algo_names(nncf_network, nncf_config,
+                                                                              algo_names=[algo_name])
+            compression_ctrl.multi_elasticity_handler._weights_calc = WeightsFlopsCalculatorForTransformer(
+                                                                            config.num_attention_heads,
+                                                                      )
+            return compression_ctrl, model
+
         # Dispatch model with hooks on all devices if necessary
         if device_map is not None:
             dispatch_model(model, device_map=device_map, offload_dir=offload_folder)
diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index 11664f66c..ee37ddd2d 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -268,7 +268,7 @@ class BertSelfAttention(nn.Module):
         self.is_decoder = config.is_decoder
 
     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, -1)
         x = x.view(new_x_shape)
         return x.permute(0, 2, 1, 3)
 
@@ -357,7 +357,7 @@ class BertSelfAttention(nn.Module):
         context_layer = torch.matmul(attention_probs, value_layer)
 
         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
+        new_context_layer_shape = context_layer.size()[:-2] + (-1,)
         context_layer = context_layer.view(new_context_layer_shape)
 
         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
diff --git a/src/transformers/models/roberta/modeling_roberta.py b/src/transformers/models/roberta/modeling_roberta.py
index d27fc5fa3..885a6925c 100644
--- a/src/transformers/models/roberta/modeling_roberta.py
+++ b/src/transformers/models/roberta/modeling_roberta.py
@@ -179,7 +179,7 @@ class RobertaSelfAttention(nn.Module):
         self.is_decoder = config.is_decoder
 
     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, -1)
         x = x.view(new_x_shape)
         return x.permute(0, 2, 1, 3)
 
@@ -268,7 +268,7 @@ class RobertaSelfAttention(nn.Module):
         context_layer = torch.matmul(attention_probs, value_layer)
 
         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
+        new_context_layer_shape = context_layer.size()[:-2] + (-1,)
         context_layer = context_layer.view(new_context_layer_shape)
 
         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
diff --git a/src/transformers/optimization.py b/src/transformers/optimization.py
index b957acb6d..eba0ba733 100644
--- a/src/transformers/optimization.py
+++ b/src/transformers/optimization.py
@@ -22,6 +22,7 @@ import torch
 from torch import nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import LambdaLR
+from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import StageDescriptor
 
 from .trainer_utils import SchedulerType
 from .utils import logging
@@ -31,6 +32,27 @@ from .utils.versions import require_version
 logger = logging.get_logger(__name__)
 
 
+class BNASLambdaLR(LambdaLR):
+    def __init__(self, optimizer: Optimizer, lr_lambda: Callable[[int], float],
+                 last_epoch: int = -1, verbose: bool = False):
+        self._current_step = -1
+        super().__init__(optimizer, lr_lambda, last_epoch, verbose)
+
+    @property
+    def current_step(self) -> int:
+        return self._current_step
+
+    def step(self, epoch: int = None) -> None:
+        super().step(epoch)
+        self._current_step += 1
+
+    def stage_step(self, stage_desc: StageDescriptor) -> None:
+        pass
+
+    def epoch_step(self, next_epoch: Optional[int] = None) -> None:
+        pass
+
+
 def get_constant_schedule(optimizer: Optimizer, last_epoch: int = -1):
     """
     Create a schedule with a constant learning rate, using the learning rate set in optimizer.
@@ -44,7 +66,7 @@ def get_constant_schedule(optimizer: Optimizer, last_epoch: int = -1):
     Return:
         `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.
     """
-    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)
+    return BNASLambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)
 
 
 def get_constant_schedule_with_warmup(optimizer: Optimizer, num_warmup_steps: int, last_epoch: int = -1):
@@ -69,7 +91,7 @@ def get_constant_schedule_with_warmup(optimizer: Optimizer, num_warmup_steps: in
             return float(current_step) / float(max(1.0, num_warmup_steps))
         return 1.0
 
-    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)
+    return BNASLambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)
 
 
 def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):
@@ -98,7 +120,7 @@ def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_st
             0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
         )
 
-    return LambdaLR(optimizer, lr_lambda, last_epoch)
+    return BNASLambdaLR(optimizer, lr_lambda, last_epoch)
 
 
 def get_cosine_schedule_with_warmup(
@@ -132,7 +154,7 @@ def get_cosine_schedule_with_warmup(
         progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
         return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
 
-    return LambdaLR(optimizer, lr_lambda, last_epoch)
+    return BNASLambdaLR(optimizer, lr_lambda, last_epoch)
 
 
 def get_cosine_with_hard_restarts_schedule_with_warmup(
@@ -167,7 +189,7 @@ def get_cosine_with_hard_restarts_schedule_with_warmup(
             return 0.0
         return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))
 
-    return LambdaLR(optimizer, lr_lambda, last_epoch)
+    return BNASLambdaLR(optimizer, lr_lambda, last_epoch)
 
 
 def get_polynomial_decay_schedule_with_warmup(
@@ -217,7 +239,7 @@ def get_polynomial_decay_schedule_with_warmup(
             decay = lr_range * pct_remaining**power + lr_end
             return decay / lr_init  # as LambdaLR multiplies by lr_init
 
-    return LambdaLR(optimizer, lr_lambda, last_epoch)
+    return BNASLambdaLR(optimizer, lr_lambda, last_epoch)
 
 
 TYPE_TO_SCHEDULER_FUNCTION = {
@@ -623,7 +645,7 @@ class Adafactor(Optimizer):
         return loss
 
 
-class AdafactorSchedule(LambdaLR):
+class AdafactorSchedule(BNASLambdaLR):
     """
     Since [`~optimization.Adafactor`] performs its own scheduling, if the training loop relies on a scheduler (e.g.,
     for logging), this class creates a proxy object that retrieves the current lr values from the optimizer.
diff --git a/src/transformers/pytorch_utils.py b/src/transformers/pytorch_utils.py
index d94e049b5..0a5741dbd 100644
--- a/src/transformers/pytorch_utils.py
+++ b/src/transformers/pytorch_utils.py
@@ -88,6 +88,8 @@ def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0)
     return new_layer
 
 
+import nncf
+@nncf.torch.register_module()
 class Conv1D(nn.Module):
     """
     1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 214e7a978..f680296f5 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -29,11 +29,12 @@ import sys
 import time
 import warnings
 from collections.abc import Mapping
+from functools import partial
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
 
 from tqdm.auto import tqdm
-
+from nncf.torch.nncf_network import NNCFNetwork
 
 # Integrations must be imported before ML frameworks:
 from .integrations import (  # isort: split
@@ -55,6 +56,8 @@ import numpy as np
 import torch
 import torch.distributed as dist
 from packaging import version
+from nncf.api.compression import CompressionStage
+from nncf.experimental.torch.nas.bootstrapNAS.training.progressive_shrinking_controller import ProgressiveShrinkingController
 from torch import nn
 from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
 from torch.utils.data.distributed import DistributedSampler
@@ -132,6 +135,7 @@ from .trainer_utils import (
 from .training_args import OptimizerNames, ParallelMode, TrainingArguments
 from .utils import (
     CONFIG_NAME,
+    NNCF_CONFIG_NAME,
     WEIGHTS_INDEX_NAME,
     WEIGHTS_NAME,
     find_labels,
@@ -199,6 +203,7 @@ logger = logging.get_logger(__name__)
 
 
 # Name of the files used for checkpointing
+ELASTICITY_STATE = "elasticity_state.pt"
 TRAINING_ARGS_NAME = "training_args.bin"
 TRAINER_STATE_NAME = "trainer_state.json"
 OPTIMIZER_NAME = "optimizer.pt"
@@ -304,12 +309,15 @@ class Trainer:
         callbacks: Optional[List[TrainerCallback]] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
         preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None,
+        compression_ctrl: ProgressiveShrinkingController = None
     ):
         if args is None:
             output_dir = "tmp_trainer"
             logger.info(f"No `TrainingArguments` passed, using `output_dir={output_dir}`.")
             args = TrainingArguments(output_dir=output_dir)
         self.args = args
+
+        self.compression_ctrl = compression_ctrl
         # Seed must be set before instantiating the model when using model
         enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)
         self.hp_name = None
@@ -623,7 +631,10 @@ class Trainer:
         self.current_flos = 0
         self.hp_search_backend = None
         self.use_tune_checkpoints = False
-        default_label_names = find_labels(self.model.__class__)
+        model_class = self.model.__class__
+        if isinstance(self.model, NNCFNetwork):
+            model_class = self.model.get_nncf_wrapped_model().__class__
+        default_label_names = find_labels(model_class)
         self.label_names = default_label_names if self.args.label_names is None else self.args.label_names
         self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)
 
@@ -708,7 +719,10 @@ class Trainer:
     def _set_signature_columns_if_needed(self):
         if self._signature_columns is None:
             # Inspect model forward signature to keep only the arguments it accepts.
-            signature = inspect.signature(self.model.forward)
+            if isinstance(self.model, NNCFNetwork):
+                signature = inspect.signature(self.model.get_nncf_wrapped_model().forward)
+            else:
+                signature = inspect.signature(self.model.forward)
             self._signature_columns = list(signature.parameters.keys())
             # Labels may be named label or label_ids, the default data collator handles that.
             self._signature_columns += list(set(["label", "label_ids"] + self.label_names))
@@ -1144,6 +1158,8 @@ class Trainer:
                 num_warmup_steps=self.args.get_warmup_steps(num_training_steps),
                 num_training_steps=num_training_steps,
             )
+        if self.compression_ctrl is not None:
+            self.compression_ctrl.scheduler.lr_scheduler = self.lr_scheduler
         return self.lr_scheduler
 
     def num_examples(self, dataloader: DataLoader) -> int:
@@ -1231,6 +1247,9 @@ class Trainer:
                 self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
                 torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
+                if self.compression_ctrl is not None:
+                    elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+                    torch.save(elasticity_state, os.path.join(output_dir, ELASTICITY_STATE))
 
     def call_model_init(self, trial=None):
         model_init_argcount = number_of_arguments(self.model_init)
@@ -1409,6 +1428,8 @@ class Trainer:
 
             if self.args.ddp_bucket_cap_mb is not None:
                 kwargs["bucket_cap_mb"] = self.args.ddp_bucket_cap_mb
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
             model = nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank] if self.args._n_gpu != 0 else None,
@@ -1517,6 +1538,16 @@ class Trainer:
         # total number of training steps to execute: max_steps
         total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size
 
+        if self.compression_ctrl is not None:
+            #save nncf_config
+            nncf_config_save_path = os.path.join(args.output_dir, NNCF_CONFIG_NAME)
+            shutil.copyfile(args.nncf_config, nncf_config_save_path)
+
+            self.best_compression_stage = CompressionStage.UNCOMPRESSED
+
+            if not has_length(train_dataloader):
+                raise RuntimeError("BootstrapNAS requires dataloader to have a length")
+
         len_dataloader = None
         if has_length(train_dataloader):
             len_dataloader = len(train_dataloader)
@@ -1687,6 +1718,10 @@ class Trainer:
                     _ = list(train_dataloader.sampler)
 
         for epoch in range(epochs_trained, num_train_epochs):
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.scheduler.epoch_step()
+                self.compression_stage = self.compression_ctrl.compression_stage()
+                logger.info(self.compression_ctrl.multi_elasticity_handler.get_search_space())
             if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                 train_dataloader.sampler.set_epoch(epoch)
             elif hasattr(train_dataloader, "dataset") and isinstance(train_dataloader.dataset, IterableDatasetShard):
@@ -1790,6 +1825,8 @@ class Trainer:
                             )
 
                     # Optimizer step
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.step()
                     optimizer_was_run = True
                     if self.deepspeed:
                         pass  # called outside the loop
@@ -1811,7 +1848,7 @@ class Trainer:
                     if optimizer_was_run and not self.deepspeed:
                         self.lr_scheduler.step()
 
-                    model.zero_grad()
+                    model.zero_grad(set_to_none=True)
                     self.state.global_step += 1
                     self.state.epoch = epoch + (step + 1) / steps_in_epoch
                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
@@ -1832,6 +1869,8 @@ class Trainer:
 
             self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
             self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
+            if self.compression_ctrl is not None:
+                self.best_compression_stage = max(self.compression_stage, self.best_compression_stage)
 
             if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
                 if is_torch_tpu_available():
@@ -1878,6 +1917,9 @@ class Trainer:
 
         self.control = self.callback_handler.on_train_end(args, self.state, self.control)
 
+        if self.compression_ctrl is not None:
+            return TrainOutput(self.state.global_step, train_loss, metrics), model, self.compression_ctrl.elasticity_controller
+
         return TrainOutput(self.state.global_step, train_loss, metrics)
 
     def _load_from_checkpoint(self, resume_from_checkpoint, model=None):
@@ -2040,20 +2082,36 @@ class Trainer:
             self.log(logs)
 
         metrics = None
+        metrics_minsubnet = None
+        metrics_supernet = None
         if self.control.should_evaluate:
-            if isinstance(self.eval_dataset, dict):
-                for eval_dataset_name, eval_dataset in self.eval_dataset.items():
-                    metrics = self.evaluate(
-                        eval_dataset=eval_dataset,
-                        ignore_keys=ignore_keys_for_eval,
-                        metric_key_prefix=f"eval_{eval_dataset_name}",
-                    )
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.multi_elasticity_handler.activate_minimum_subnet()
+                active_subnet = self.compression_ctrl.multi_elasticity_handler.get_active_config()
+                logger.info(f'Minimum SubNet={active_subnet}')
+                metrics_minsubnet = self.evaluate(ignore_keys=ignore_keys_for_eval, active_subnet={'Minimum SubNet': str(active_subnet)})
+                self._report_to_hp_search(trial, epoch, metrics_minsubnet)
+
+                self.compression_ctrl.multi_elasticity_handler.activate_supernet()
+                active_subnet = self.compression_ctrl.multi_elasticity_handler.get_active_config()
+                logger.info(f'SuperNet={active_subnet}')
+                metrics_supernet = self.evaluate(ignore_keys=ignore_keys_for_eval, active_subnet={'SuperNet': str(active_subnet)})
+                self._report_to_hp_search(trial, epoch, metrics_supernet)
             else:
-                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
-            self._report_to_hp_search(trial, self.state.global_step, metrics)
+                if isinstance(self.eval_dataset, dict):
+                    for eval_dataset_name, eval_dataset in self.eval_dataset.items():
+                        metrics = self.evaluate(
+                            eval_dataset=eval_dataset,
+                            ignore_keys=ignore_keys_for_eval,
+                            metric_key_prefix=f"eval_{eval_dataset_name}",
+                        )
+                else:
+                    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
+                self._report_to_hp_search(trial, self.state.global_step, metrics)
 
         if self.control.should_save:
-            self._save_checkpoint(model, trial, metrics=metrics)
+            self._save_checkpoint(model, trial, metrics=metrics, metrics_minsubnet=metrics_minsubnet,
+                                  metrics_supernet=metrics_supernet)
             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
 
     def _load_rng_state(self, checkpoint):
@@ -2097,7 +2155,13 @@ class Trainer:
         if is_torch_tpu_available():
             xm.set_rng_state(checkpoint_rng_state["xla"])
 
-    def _save_checkpoint(self, model, trial, metrics=None):
+    def is_bootstrapNAS_best_accuracy(self, metric_value, best_metric_value, operator,
+                                         compression_stage, best_compression_stage):
+        is_best_by_accuracy = operator(metric_value, best_metric_value) and compression_stage == best_compression_stage
+        is_best = is_best_by_accuracy or compression_stage > best_compression_stage
+        return is_best
+
+    def _save_checkpoint(self, model, trial, metrics=None,  metrics_minsubnet=None, metrics_supernet=None):
         # In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we
         # want to save except FullyShardedDDP.
         # assert unwrap_model(model) is self.model, "internal model should be a reference to self.model"
@@ -2182,9 +2246,42 @@ class Trainer:
                 self.state.best_metric = metric_value
                 self.state.best_model_checkpoint = output_dir
 
+        #BootstrapNAS state
+        if (
+            self.compression_ctrl is not None and self.args.metric_for_best_model is not None
+            and metrics_supernet is not None and metrics_minsubnet is not None
+        ):
+            metric_to_check = self.args.metric_for_best_model
+            if not metric_to_check.startswith("eval_"):
+                metric_to_check = f"eval_{metric_to_check}"
+            metric_supernet_value = metrics_supernet[metric_to_check]
+            metrics_minsubnet_value = metrics_minsubnet[metric_to_check]
+
+            self.state.supernet_acc = metric_supernet_value
+            self.state.min_subnet_acc = metrics_minsubnet_value
+
+            operator = np.greater if self.args.greater_is_better else np.less
+            if(
+                self.state.supernet_best_acc is None
+                or self.state.best_supernet_model_checkpoint is None
+                or self.is_bootstrapNAS_best_accuracy(metric_supernet_value, self.state.supernet_best_acc, operator,
+                                                          self.compression_stage, self.best_compression_stage)
+            ):
+                self.state.supernet_best_acc = metric_supernet_value
+                self.state.best_supernet_model_checkpoint = output_dir
+            if(
+                self.state.min_subnet_best_acc is None
+                or self.is_bootstrapNAS_best_accuracy(metrics_minsubnet_value, self.state.min_subnet_best_acc, operator,
+                                                          self.compression_stage, self.best_compression_stage)
+            ):
+                self.state.min_subnet_best_acc = metrics_minsubnet_value
+
         # Save the Trainer state
         if self.args.should_save:
             self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
+            if self.compression_ctrl is not None:
+                elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+                torch.save(elasticity_state, os.path.join(output_dir, ELASTICITY_STATE))
 
         # Save RNG state in non-distributed training
         rng_states = {
@@ -2657,10 +2754,15 @@ class Trainer:
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
+            unwrapped_model = unwrap_model(self.model)
+            if isinstance(unwrapped_model, NNCFNetwork):
+                is_pretrained = isinstance(unwrapped_model.get_nncf_wrapped_model(), PreTrainedModel)
+            else:
+                is_pretrained = isinstance(unwrapped_model, PreTrainedModel)
+            if is_pretrained:
                 if state_dict is None:
-                    state_dict = self.model.state_dict()
-                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
+                    state_dict = unwrapped_model.state_dict()
+                unwrapped_model.save_pretrained(output_dir, state_dict=state_dict)
             else:
                 logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                 if state_dict is None:
@@ -2671,6 +2773,10 @@ class Trainer:
         if self.tokenizer is not None:
             self.tokenizer.save_pretrained(output_dir)
 
+        if self.compression_ctrl is not None:
+            elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+            torch.save(elasticity_state, os.path.join(output_dir, ELASTICITY_STATE))
+
         # Good practice: save your training arguments together with the trained model
         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
 
@@ -2739,6 +2845,7 @@ class Trainer:
         eval_dataset: Optional[Dataset] = None,
         ignore_keys: Optional[List[str]] = None,
         metric_key_prefix: str = "eval",
+        active_subnet: Dict[str, str] = None
     ) -> Dict[str, float]:
         """
         Run evaluation and returns metrics.
@@ -2781,6 +2888,9 @@ class Trainer:
             metric_key_prefix=metric_key_prefix,
         )
 
+        if active_subnet is not None:
+            output.metrics.update(active_subnet)
+
         total_batch_size = self.args.eval_batch_size * self.args.world_size
         output.metrics.update(
             speed_metrics(
diff --git a/src/transformers/trainer_callback.py b/src/transformers/trainer_callback.py
index 8749e5f3f..7fc5471d4 100644
--- a/src/transformers/trainer_callback.py
+++ b/src/transformers/trainer_callback.py
@@ -88,6 +88,12 @@ class TrainerState:
     trial_name: str = None
     trial_params: Dict[str, Union[str, float, int, bool]] = None
 
+    #BootstrapNAS
+    supernet_best_acc: float = None
+    supernet_acc: float = None
+    min_subnet_best_acc: float = None
+    min_subnet_acc: float = None
+
     def __post_init__(self):
         if self.log_history is None:
             self.log_history = []
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 170315fe2..9e7df17fc 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -23,6 +23,9 @@ from enum import Enum
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Union
 
+from nncf.config.config import NNCFConfig
+from nncf.config.structures import NNCFExtraConfigStruct
+from nncf.experimental.torch.nas.bootstrapNAS.training.scheduler import DEFAULT_STAGE_LR_RATE
 from packaging import version
 
 from .debug_utils import DebugOption
@@ -137,6 +140,8 @@ class TrainingArguments:
             Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used
             by your training/evaluation scripts instead. See the [example
             scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
+        do_search (:obj:`bool`, `optional`, defaults to :obj:`False`):
+            Whether to run BootstrapNAS searching or not.
         do_eval (`bool`, *optional*):
             Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is
             different from `"no"`. This argument is not directly used by [`Trainer`], it's intended to be used by your
@@ -514,6 +519,7 @@ class TrainingArguments:
     )
 
     do_train: bool = field(default=False, metadata={"help": "Whether to run training."})
+    do_search: bool = field(default=False, metadata={"help": "Whether to run BootstrapNAS search."})
     do_eval: bool = field(default=False, metadata={"help": "Whether to run eval on the dev set."})
     do_predict: bool = field(default=False, metadata={"help": "Whether to run predictions on the test set."})
     evaluation_strategy: Union[IntervalStrategy, str] = field(
@@ -993,6 +999,12 @@ class TrainingArguments:
         },
     )
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
+    to_onnx: str = field(default=None,
+                         metadata={"help": "Name of the ONNX model file to export the model to."})
+
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
         # This needs to happen before any call to self.device or self.n_gpu.
@@ -1452,7 +1464,9 @@ class TrainingArguments:
                 device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                 # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at
                 # the default value.
-                self._n_gpu = torch.cuda.device_count()
+                self._n_gpu = torch.cuda.device_count() if self.nncf_config is None else 1
+                if self.nncf_config is not None and torch.cuda.device_count() > 1:
+                    logger.warning("Currently BootstrapNAS only supports 1 GPU/distributed training")
         else:
             # Here, we'll use torch.distributed.
             # Initializes the distributed backend which will take care of synchronizing nodes/GPUs
@@ -1730,3 +1744,45 @@ class ParallelMode(Enum):
     SAGEMAKER_MODEL_PARALLEL = "sagemaker_model_parallel"
     SAGEMAKER_DATA_PARALLEL = "sagemaker_data_parallel"
     TPU = "tpu"
+
+
+class BNASLRSchedulerArgs(NNCFExtraConfigStruct):
+    def __init__(self, nncf_config):
+        schedule_config = nncf_config.get('bootstrapNAS' ,{}).get('training', {})\
+            .get('schedule', {}).get('list_stage_descriptions', [])
+
+        if len(schedule_config) != 1:
+            raise RuntimeError("Please use one-stage training")
+        self._learning_rate = schedule_config[0].get('init_lr', DEFAULT_STAGE_LR_RATE)
+        self._num_train_epochs = schedule_config[0].get('epochs', 1)
+        self._lr_scheduler_type = SchedulerType('cosine')
+
+    @property
+    def learning_rate(self) -> float:
+        return self._learning_rate
+
+    @property
+    def num_train_epochs(self) -> float:
+        return self._num_train_epochs
+
+    @property
+    def lr_scheduler_type(self) -> str:
+        return self._lr_scheduler_type
+
+    @classmethod
+    def get_id(cls) -> str:
+        return "bootstrapnas_lr_scheduler_args"
+
+
+def update_bnas_lr_scheduler_kwargs(nncf_config: NNCFConfig, training_args: TrainingArguments):
+    try:
+        args = nncf_config.get_extra_struct(BNASLRSchedulerArgs)
+    except KeyError:
+        raise RuntimeError(
+            'Unable to create the learning rate scheduler for BootstrapNAS '
+            'because the data loader is not provided as an extra struct. Refer to the '
+            '`NNCFConfig.register_extra_structs` method and the `BNASLRSchedulerArgs` class.') from None
+    training_args.learning_rate = args.learning_rate
+    training_args.num_train_epochs = args.num_train_epochs
+    training_args.lr_scheduler_type = args.lr_scheduler_type
+    training_args.max_steps = -1
diff --git a/src/transformers/utils/__init__.py b/src/transformers/utils/__init__.py
index 2269f2254..4aa93202a 100644
--- a/src/transformers/utils/__init__.py
+++ b/src/transformers/utils/__init__.py
@@ -22,6 +22,7 @@
 from packaging import version
 
 from .. import __version__
+from .bnas_utils import WeightsFlopsCalculatorForTransformer
 from .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
 from .doc import (
     add_code_sample_docstrings,
@@ -163,6 +164,7 @@ FLAX_WEIGHTS_INDEX_NAME = "flax_model.msgpack.index.json"
 SAFE_WEIGHTS_NAME = "model.safetensors"
 SAFE_WEIGHTS_INDEX_NAME = "model.safetensors.index.json"
 CONFIG_NAME = "config.json"
+NNCF_CONFIG_NAME = "nncf_config.json"
 FEATURE_EXTRACTOR_NAME = "preprocessor_config.json"
 MODEL_CARD_NAME = "modelcard.json"
 
diff --git a/src/transformers/utils/bnas_utils.py b/src/transformers/utils/bnas_utils.py
new file mode 100644
index 000000000..4631890b5
--- /dev/null
+++ b/src/transformers/utils/bnas_utils.py
@@ -0,0 +1,97 @@
+"""
+ Copyright (c) 2022 Intel Corporation
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+      http://www.apache.org/licenses/LICENSE-2.0
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+"""
+
+from typing import Dict
+from typing import List
+from typing import Tuple
+
+from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNodeName
+from nncf.common.graph.operator_metatypes import OperatorMetatype
+from nncf.torch.graph.operator_metatypes import PTLinearMetatype
+from nncf.torch.graph.operator_metatypes import PTMatMulMetatype
+from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
+
+
+class WeightsFlopsCalculatorForTransformer(WeightsFlopsCalculator):
+    """
+    Collection of weight and flops calculation functions.
+    Class instance keeps only parameters that are constant during
+    compression algorithms execution.
+    """
+
+    def __init__(self,
+                 n_heads: int,
+                 linear_op_metatypes: List[OperatorMetatype] = [PTLinearMetatype],
+                 matmul_op_metatypes: List[OperatorMetatype] = [PTMatMulMetatype]):
+        """
+        Constructor.
+
+        :param n_heads: number of attention heads.
+        :param linear_op_metatypes: List of metatypes defining linear/fully connected operations.
+        :param matmul_op_metatypes: List of metatypes defining matmul operations.
+        """
+        super().__init__([], linear_op_metatypes)
+        self._n_heads = n_heads
+        self._matmul_op_metatype = matmul_op_metatypes
+
+    def count_flops_and_weights_per_node(self,
+                                         graph: NNCFGraph,
+                                         output_shapes: Dict[NNCFNodeName, int],
+                                         input_channels: Dict[NNCFNodeName, int] = None,
+                                         output_channels: Dict[NNCFNodeName, int] = None,
+                                         kernel_sizes: Dict[NNCFNodeName, Tuple[int, int]] = None,
+                                         op_addresses_to_skip: List[NNCFNodeName] = None) -> \
+        Tuple[Dict[NNCFNodeName, int], Dict[NNCFNodeName, int]]:
+        """
+        Counts the number of weights and FLOPs per node in the model for convolution and fully connected layers.
+
+        :param graph: NNCFGraph.
+        :param output_shapes: Dictionary of output dimension shapes for convolutions and
+            fully connected layers. E.g {node_name: (height, width)}
+        :param input_channels: Dictionary of input channels number in convolutions.
+            If not specified, taken from the graph. {node_name: channels_num}
+        :param output_channels: Dictionary of output channels number in convolutions.
+            If not specified, taken from the graph. {node_name: channels_num}
+        :param kernel_sizes: Dictionary of kernel sizes in convolutions.
+            If not specified, taken from the graph. {node_name: kernel_size}.
+            It's only supposed to be used in NAS in case of Elastic Kernel enabled.
+        :param op_addresses_to_skip: List of operation addresses of layers that should be skipped from calculation.
+            It's only supposed to be used in NAS in case of Elastic Depth enabled.
+        :return Dictionary of FLOPs number {node_name: flops_num}
+                Dictionary of weights number {node_name: weights_num}
+        """
+        input_channels = input_channels or {}
+        output_channels = output_channels or {}
+        kernel_sizes = kernel_sizes or {}
+        op_addresses_to_skip = op_addresses_to_skip or []
+
+        flops, weights = super().count_flops_and_weights_per_node(graph, output_shapes, input_channels, output_channels,
+                                                                  kernel_sizes, op_addresses_to_skip)
+
+        for node in graph.get_nodes_by_metatypes(self._matmul_op_metatype):
+            name = node.node_name
+            if name in op_addresses_to_skip:
+                continue
+
+            # get previous qkv nodes
+            input_nodes = graph.get_previous_nodes(node)
+            while(len(input_nodes) > 0 and input_nodes[0].node_type != 'linear'):
+                input_nodes = graph.get_previous_nodes(input_nodes[0])
+            qkv_len = output_shapes[input_nodes[0].node_name][1]
+            qkv_head_dim = output_channels[input_nodes[0].node_name] // self._n_heads
+
+            flops[name] = 2 * self._n_heads * qkv_len * qkv_len * qkv_head_dim
+            weights[name] = 0
+
+        return flops, weights
-- 
2.17.1

